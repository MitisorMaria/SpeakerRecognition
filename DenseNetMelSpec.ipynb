{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNetMelSpec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Iz1IL-LsFkMo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df78b511-a98c-4436-d691-68d91753adea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ueQE1_TFpM4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "05e87075-4b03-4747-dc27-4e497bf28e90"
      },
      "source": [
        "cd /content/gdrive/My Drive/Colab Notebooks/wav"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Untitled folder/wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cPoGrpzNahan",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b37a051-5866-475e-90d5-688235de1b75"
      },
      "source": [
        "import signal_processing, data_load, learn, evaluate, render"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UkCE6NXdeZ2e",
        "colab": {}
      },
      "source": [
        "num_rows = 32\n",
        "num_columns = 110\n",
        "num_channels = 3\n",
        "\n",
        "num_epochs = 100\n",
        "num_batch_size = 13\n",
        "num_speakers = 10\n",
        "hop_length = 1024\n",
        "nr_mels = 32\n",
        "n_fft = 2048\n",
        "num_seconds = 5.0\n",
        "\n",
        "fine_tune_at = 300\n",
        "base_lr = 0.0001"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SRpzC9PxtFjA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5958e9e3-73f8-46ab-e297-b841cab68389"
      },
      "source": [
        "featuresdf = data_load.make_dataframe_melspectrogram(num_speakers, './', num_rows, n_fft, hop_length, num_columns, num_seconds)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished feature extraction from  970  files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ttUQbv2rtFjD",
        "colab": {}
      },
      "source": [
        "result_sets = data_load.make_train_test_sets(featuresdf, num_rows, num_columns, num_channels)\n",
        "num_labels = result_sets[4]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yC2UFaTPtFjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b73c7c7-ec8c-46c9-897a-dc0f5e1fa2bb"
      },
      "source": [
        "base_model = learn.get_densenet(num_rows, num_columns, num_channels, num_labels, fine_tune_at)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 0s 0us/step\n",
            "Number of layers in the base model:  428\n",
            "Model: \"densenet121\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 110, 3)] 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2D)  (None, 38, 116, 3)   0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/conv (Conv2D)             (None, 16, 55, 64)   9408        zero_padding2d[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv1/bn (BatchNormalization)   (None, 16, 55, 64)   256         conv1/conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1/relu (Activation)         (None, 16, 55, 64)   0           conv1/bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_1 (ZeroPadding2D (None, 18, 57, 64)   0           conv1/relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 8, 28, 64)    0           zero_padding2d_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 8, 28, 64)    256         pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_relu (Activation (None, 8, 28, 64)    0           conv2_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 8, 28, 128)   8192        conv2_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 8, 28, 128)   512         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 8, 28, 128)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 8, 28, 32)    36864       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_concat (Concatenat (None, 8, 28, 96)    0           pool1[0][0]                      \n",
            "                                                                 conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_bn (BatchNormali (None, 8, 28, 96)    384         conv2_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_0_relu (Activation (None, 8, 28, 96)    0           conv2_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 8, 28, 128)   12288       conv2_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 8, 28, 128)   512         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 8, 28, 128)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 8, 28, 32)    36864       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_concat (Concatenat (None, 8, 28, 128)   0           conv2_block1_concat[0][0]        \n",
            "                                                                 conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_bn (BatchNormali (None, 8, 28, 128)   512         conv2_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_0_relu (Activation (None, 8, 28, 128)   0           conv2_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 8, 28, 128)   16384       conv2_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 8, 28, 128)   512         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 8, 28, 128)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 8, 28, 32)    36864       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_concat (Concatenat (None, 8, 28, 160)   0           conv2_block2_concat[0][0]        \n",
            "                                                                 conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_bn (BatchNormali (None, 8, 28, 160)   640         conv2_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_0_relu (Activation (None, 8, 28, 160)   0           conv2_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_conv (Conv2D)    (None, 8, 28, 128)   20480       conv2_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_bn (BatchNormali (None, 8, 28, 128)   512         conv2_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_1_relu (Activation (None, 8, 28, 128)   0           conv2_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_2_conv (Conv2D)    (None, 8, 28, 32)    36864       conv2_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block4_concat (Concatenat (None, 8, 28, 192)   0           conv2_block3_concat[0][0]        \n",
            "                                                                 conv2_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_bn (BatchNormali (None, 8, 28, 192)   768         conv2_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_0_relu (Activation (None, 8, 28, 192)   0           conv2_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_conv (Conv2D)    (None, 8, 28, 128)   24576       conv2_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_bn (BatchNormali (None, 8, 28, 128)   512         conv2_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_1_relu (Activation (None, 8, 28, 128)   0           conv2_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_2_conv (Conv2D)    (None, 8, 28, 32)    36864       conv2_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block5_concat (Concatenat (None, 8, 28, 224)   0           conv2_block4_concat[0][0]        \n",
            "                                                                 conv2_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_bn (BatchNormali (None, 8, 28, 224)   896         conv2_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_0_relu (Activation (None, 8, 28, 224)   0           conv2_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_conv (Conv2D)    (None, 8, 28, 128)   28672       conv2_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_bn (BatchNormali (None, 8, 28, 128)   512         conv2_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_1_relu (Activation (None, 8, 28, 128)   0           conv2_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_2_conv (Conv2D)    (None, 8, 28, 32)    36864       conv2_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block6_concat (Concatenat (None, 8, 28, 256)   0           conv2_block5_concat[0][0]        \n",
            "                                                                 conv2_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_bn (BatchNormalization)   (None, 8, 28, 256)   1024        conv2_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "pool2_relu (Activation)         (None, 8, 28, 256)   0           pool2_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool2_conv (Conv2D)             (None, 8, 28, 128)   32768       pool2_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool2_pool (AveragePooling2D)   (None, 4, 14, 128)   0           pool2_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 4, 14, 128)   512         pool2_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_relu (Activation (None, 4, 14, 128)   0           conv3_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 4, 14, 128)   16384       conv3_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 4, 14, 128)   512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 4, 14, 128)   0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 4, 14, 32)    36864       conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_concat (Concatenat (None, 4, 14, 160)   0           pool2_pool[0][0]                 \n",
            "                                                                 conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_bn (BatchNormali (None, 4, 14, 160)   640         conv3_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_0_relu (Activation (None, 4, 14, 160)   0           conv3_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 4, 14, 128)   20480       conv3_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 4, 14, 128)   512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 4, 14, 128)   0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 4, 14, 32)    36864       conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_concat (Concatenat (None, 4, 14, 192)   0           conv3_block1_concat[0][0]        \n",
            "                                                                 conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_bn (BatchNormali (None, 4, 14, 192)   768         conv3_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_0_relu (Activation (None, 4, 14, 192)   0           conv3_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 4, 14, 128)   24576       conv3_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 4, 14, 128)   512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 4, 14, 128)   0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 4, 14, 32)    36864       conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_concat (Concatenat (None, 4, 14, 224)   0           conv3_block2_concat[0][0]        \n",
            "                                                                 conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_bn (BatchNormali (None, 4, 14, 224)   896         conv3_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_0_relu (Activation (None, 4, 14, 224)   0           conv3_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 4, 14, 128)   28672       conv3_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 4, 14, 128)   512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 4, 14, 128)   0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 4, 14, 32)    36864       conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_concat (Concatenat (None, 4, 14, 256)   0           conv3_block3_concat[0][0]        \n",
            "                                                                 conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_bn (BatchNormali (None, 4, 14, 256)   1024        conv3_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_0_relu (Activation (None, 4, 14, 256)   0           conv3_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_conv (Conv2D)    (None, 4, 14, 128)   32768       conv3_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_bn (BatchNormali (None, 4, 14, 128)   512         conv3_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_1_relu (Activation (None, 4, 14, 128)   0           conv3_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_2_conv (Conv2D)    (None, 4, 14, 32)    36864       conv3_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block5_concat (Concatenat (None, 4, 14, 288)   0           conv3_block4_concat[0][0]        \n",
            "                                                                 conv3_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_bn (BatchNormali (None, 4, 14, 288)   1152        conv3_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_0_relu (Activation (None, 4, 14, 288)   0           conv3_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_conv (Conv2D)    (None, 4, 14, 128)   36864       conv3_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_bn (BatchNormali (None, 4, 14, 128)   512         conv3_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_1_relu (Activation (None, 4, 14, 128)   0           conv3_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_2_conv (Conv2D)    (None, 4, 14, 32)    36864       conv3_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block6_concat (Concatenat (None, 4, 14, 320)   0           conv3_block5_concat[0][0]        \n",
            "                                                                 conv3_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_bn (BatchNormali (None, 4, 14, 320)   1280        conv3_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_0_relu (Activation (None, 4, 14, 320)   0           conv3_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_conv (Conv2D)    (None, 4, 14, 128)   40960       conv3_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_bn (BatchNormali (None, 4, 14, 128)   512         conv3_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_1_relu (Activation (None, 4, 14, 128)   0           conv3_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_2_conv (Conv2D)    (None, 4, 14, 32)    36864       conv3_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block7_concat (Concatenat (None, 4, 14, 352)   0           conv3_block6_concat[0][0]        \n",
            "                                                                 conv3_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_bn (BatchNormali (None, 4, 14, 352)   1408        conv3_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_0_relu (Activation (None, 4, 14, 352)   0           conv3_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_conv (Conv2D)    (None, 4, 14, 128)   45056       conv3_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_bn (BatchNormali (None, 4, 14, 128)   512         conv3_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_1_relu (Activation (None, 4, 14, 128)   0           conv3_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_2_conv (Conv2D)    (None, 4, 14, 32)    36864       conv3_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block8_concat (Concatenat (None, 4, 14, 384)   0           conv3_block7_concat[0][0]        \n",
            "                                                                 conv3_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_bn (BatchNormali (None, 4, 14, 384)   1536        conv3_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_0_relu (Activation (None, 4, 14, 384)   0           conv3_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_conv (Conv2D)    (None, 4, 14, 128)   49152       conv3_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_bn (BatchNormali (None, 4, 14, 128)   512         conv3_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_1_relu (Activation (None, 4, 14, 128)   0           conv3_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_2_conv (Conv2D)    (None, 4, 14, 32)    36864       conv3_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block9_concat (Concatenat (None, 4, 14, 416)   0           conv3_block8_concat[0][0]        \n",
            "                                                                 conv3_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_bn (BatchNormal (None, 4, 14, 416)   1664        conv3_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_0_relu (Activatio (None, 4, 14, 416)   0           conv3_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_conv (Conv2D)   (None, 4, 14, 128)   53248       conv3_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_bn (BatchNormal (None, 4, 14, 128)   512         conv3_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_1_relu (Activatio (None, 4, 14, 128)   0           conv3_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_2_conv (Conv2D)   (None, 4, 14, 32)    36864       conv3_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block10_concat (Concatena (None, 4, 14, 448)   0           conv3_block9_concat[0][0]        \n",
            "                                                                 conv3_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_bn (BatchNormal (None, 4, 14, 448)   1792        conv3_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_0_relu (Activatio (None, 4, 14, 448)   0           conv3_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_conv (Conv2D)   (None, 4, 14, 128)   57344       conv3_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_bn (BatchNormal (None, 4, 14, 128)   512         conv3_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_1_relu (Activatio (None, 4, 14, 128)   0           conv3_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_2_conv (Conv2D)   (None, 4, 14, 32)    36864       conv3_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block11_concat (Concatena (None, 4, 14, 480)   0           conv3_block10_concat[0][0]       \n",
            "                                                                 conv3_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_bn (BatchNormal (None, 4, 14, 480)   1920        conv3_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_0_relu (Activatio (None, 4, 14, 480)   0           conv3_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_conv (Conv2D)   (None, 4, 14, 128)   61440       conv3_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_bn (BatchNormal (None, 4, 14, 128)   512         conv3_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_1_relu (Activatio (None, 4, 14, 128)   0           conv3_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_2_conv (Conv2D)   (None, 4, 14, 32)    36864       conv3_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block12_concat (Concatena (None, 4, 14, 512)   0           conv3_block11_concat[0][0]       \n",
            "                                                                 conv3_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_bn (BatchNormalization)   (None, 4, 14, 512)   2048        conv3_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool3_relu (Activation)         (None, 4, 14, 512)   0           pool3_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool3_conv (Conv2D)             (None, 4, 14, 256)   131072      pool3_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool3_pool (AveragePooling2D)   (None, 2, 7, 256)    0           pool3_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 2, 7, 256)    1024        pool3_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_relu (Activation (None, 2, 7, 256)    0           conv4_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 2, 7, 128)    32768       conv4_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 2, 7, 128)    512         conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 2, 7, 128)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 2, 7, 32)     36864       conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_concat (Concatenat (None, 2, 7, 288)    0           pool3_pool[0][0]                 \n",
            "                                                                 conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_bn (BatchNormali (None, 2, 7, 288)    1152        conv4_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_0_relu (Activation (None, 2, 7, 288)    0           conv4_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 2, 7, 128)    36864       conv4_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 2, 7, 128)    512         conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 2, 7, 128)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 2, 7, 32)     36864       conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_concat (Concatenat (None, 2, 7, 320)    0           conv4_block1_concat[0][0]        \n",
            "                                                                 conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_bn (BatchNormali (None, 2, 7, 320)    1280        conv4_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_0_relu (Activation (None, 2, 7, 320)    0           conv4_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 2, 7, 128)    40960       conv4_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 2, 7, 128)    512         conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 2, 7, 128)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 2, 7, 32)     36864       conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_concat (Concatenat (None, 2, 7, 352)    0           conv4_block2_concat[0][0]        \n",
            "                                                                 conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_bn (BatchNormali (None, 2, 7, 352)    1408        conv4_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_0_relu (Activation (None, 2, 7, 352)    0           conv4_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 2, 7, 128)    45056       conv4_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 2, 7, 128)    512         conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 2, 7, 128)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 2, 7, 32)     36864       conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_concat (Concatenat (None, 2, 7, 384)    0           conv4_block3_concat[0][0]        \n",
            "                                                                 conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_bn (BatchNormali (None, 2, 7, 384)    1536        conv4_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_0_relu (Activation (None, 2, 7, 384)    0           conv4_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 2, 7, 128)    49152       conv4_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 2, 7, 128)    512         conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 2, 7, 128)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 2, 7, 32)     36864       conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_concat (Concatenat (None, 2, 7, 416)    0           conv4_block4_concat[0][0]        \n",
            "                                                                 conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_bn (BatchNormali (None, 2, 7, 416)    1664        conv4_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_0_relu (Activation (None, 2, 7, 416)    0           conv4_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 2, 7, 128)    53248       conv4_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 2, 7, 128)    512         conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 2, 7, 128)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 2, 7, 32)     36864       conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_concat (Concatenat (None, 2, 7, 448)    0           conv4_block5_concat[0][0]        \n",
            "                                                                 conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_bn (BatchNormali (None, 2, 7, 448)    1792        conv4_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_0_relu (Activation (None, 2, 7, 448)    0           conv4_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_conv (Conv2D)    (None, 2, 7, 128)    57344       conv4_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_bn (BatchNormali (None, 2, 7, 128)    512         conv4_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_1_relu (Activation (None, 2, 7, 128)    0           conv4_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_2_conv (Conv2D)    (None, 2, 7, 32)     36864       conv4_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block7_concat (Concatenat (None, 2, 7, 480)    0           conv4_block6_concat[0][0]        \n",
            "                                                                 conv4_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_bn (BatchNormali (None, 2, 7, 480)    1920        conv4_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_0_relu (Activation (None, 2, 7, 480)    0           conv4_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_conv (Conv2D)    (None, 2, 7, 128)    61440       conv4_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_bn (BatchNormali (None, 2, 7, 128)    512         conv4_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_1_relu (Activation (None, 2, 7, 128)    0           conv4_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_2_conv (Conv2D)    (None, 2, 7, 32)     36864       conv4_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block8_concat (Concatenat (None, 2, 7, 512)    0           conv4_block7_concat[0][0]        \n",
            "                                                                 conv4_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_bn (BatchNormali (None, 2, 7, 512)    2048        conv4_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_0_relu (Activation (None, 2, 7, 512)    0           conv4_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_conv (Conv2D)    (None, 2, 7, 128)    65536       conv4_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_bn (BatchNormali (None, 2, 7, 128)    512         conv4_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_1_relu (Activation (None, 2, 7, 128)    0           conv4_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_2_conv (Conv2D)    (None, 2, 7, 32)     36864       conv4_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block9_concat (Concatenat (None, 2, 7, 544)    0           conv4_block8_concat[0][0]        \n",
            "                                                                 conv4_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_bn (BatchNormal (None, 2, 7, 544)    2176        conv4_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_0_relu (Activatio (None, 2, 7, 544)    0           conv4_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_conv (Conv2D)   (None, 2, 7, 128)    69632       conv4_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block10_concat (Concatena (None, 2, 7, 576)    0           conv4_block9_concat[0][0]        \n",
            "                                                                 conv4_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_bn (BatchNormal (None, 2, 7, 576)    2304        conv4_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_0_relu (Activatio (None, 2, 7, 576)    0           conv4_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_conv (Conv2D)   (None, 2, 7, 128)    73728       conv4_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block11_concat (Concatena (None, 2, 7, 608)    0           conv4_block10_concat[0][0]       \n",
            "                                                                 conv4_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_bn (BatchNormal (None, 2, 7, 608)    2432        conv4_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_0_relu (Activatio (None, 2, 7, 608)    0           conv4_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_conv (Conv2D)   (None, 2, 7, 128)    77824       conv4_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block12_concat (Concatena (None, 2, 7, 640)    0           conv4_block11_concat[0][0]       \n",
            "                                                                 conv4_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_bn (BatchNormal (None, 2, 7, 640)    2560        conv4_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_0_relu (Activatio (None, 2, 7, 640)    0           conv4_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_conv (Conv2D)   (None, 2, 7, 128)    81920       conv4_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block13_concat (Concatena (None, 2, 7, 672)    0           conv4_block12_concat[0][0]       \n",
            "                                                                 conv4_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_bn (BatchNormal (None, 2, 7, 672)    2688        conv4_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_0_relu (Activatio (None, 2, 7, 672)    0           conv4_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_conv (Conv2D)   (None, 2, 7, 128)    86016       conv4_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block14_concat (Concatena (None, 2, 7, 704)    0           conv4_block13_concat[0][0]       \n",
            "                                                                 conv4_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_bn (BatchNormal (None, 2, 7, 704)    2816        conv4_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_0_relu (Activatio (None, 2, 7, 704)    0           conv4_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_conv (Conv2D)   (None, 2, 7, 128)    90112       conv4_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block15_concat (Concatena (None, 2, 7, 736)    0           conv4_block14_concat[0][0]       \n",
            "                                                                 conv4_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_bn (BatchNormal (None, 2, 7, 736)    2944        conv4_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_0_relu (Activatio (None, 2, 7, 736)    0           conv4_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_conv (Conv2D)   (None, 2, 7, 128)    94208       conv4_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block16_concat (Concatena (None, 2, 7, 768)    0           conv4_block15_concat[0][0]       \n",
            "                                                                 conv4_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_bn (BatchNormal (None, 2, 7, 768)    3072        conv4_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_0_relu (Activatio (None, 2, 7, 768)    0           conv4_block17_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_conv (Conv2D)   (None, 2, 7, 128)    98304       conv4_block17_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block17_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block17_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block17_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block17_concat (Concatena (None, 2, 7, 800)    0           conv4_block16_concat[0][0]       \n",
            "                                                                 conv4_block17_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_bn (BatchNormal (None, 2, 7, 800)    3200        conv4_block17_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_0_relu (Activatio (None, 2, 7, 800)    0           conv4_block18_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_conv (Conv2D)   (None, 2, 7, 128)    102400      conv4_block18_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block18_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block18_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block18_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block18_concat (Concatena (None, 2, 7, 832)    0           conv4_block17_concat[0][0]       \n",
            "                                                                 conv4_block18_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_bn (BatchNormal (None, 2, 7, 832)    3328        conv4_block18_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_0_relu (Activatio (None, 2, 7, 832)    0           conv4_block19_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_conv (Conv2D)   (None, 2, 7, 128)    106496      conv4_block19_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block19_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block19_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block19_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block19_concat (Concatena (None, 2, 7, 864)    0           conv4_block18_concat[0][0]       \n",
            "                                                                 conv4_block19_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_bn (BatchNormal (None, 2, 7, 864)    3456        conv4_block19_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_0_relu (Activatio (None, 2, 7, 864)    0           conv4_block20_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_conv (Conv2D)   (None, 2, 7, 128)    110592      conv4_block20_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block20_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block20_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block20_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block20_concat (Concatena (None, 2, 7, 896)    0           conv4_block19_concat[0][0]       \n",
            "                                                                 conv4_block20_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_bn (BatchNormal (None, 2, 7, 896)    3584        conv4_block20_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_0_relu (Activatio (None, 2, 7, 896)    0           conv4_block21_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_conv (Conv2D)   (None, 2, 7, 128)    114688      conv4_block21_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block21_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block21_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block21_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block21_concat (Concatena (None, 2, 7, 928)    0           conv4_block20_concat[0][0]       \n",
            "                                                                 conv4_block21_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_bn (BatchNormal (None, 2, 7, 928)    3712        conv4_block21_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_0_relu (Activatio (None, 2, 7, 928)    0           conv4_block22_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_conv (Conv2D)   (None, 2, 7, 128)    118784      conv4_block22_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block22_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block22_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block22_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block22_concat (Concatena (None, 2, 7, 960)    0           conv4_block21_concat[0][0]       \n",
            "                                                                 conv4_block22_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_bn (BatchNormal (None, 2, 7, 960)    3840        conv4_block22_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_0_relu (Activatio (None, 2, 7, 960)    0           conv4_block23_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_conv (Conv2D)   (None, 2, 7, 128)    122880      conv4_block23_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block23_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block23_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block23_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block23_concat (Concatena (None, 2, 7, 992)    0           conv4_block22_concat[0][0]       \n",
            "                                                                 conv4_block23_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_bn (BatchNormal (None, 2, 7, 992)    3968        conv4_block23_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_0_relu (Activatio (None, 2, 7, 992)    0           conv4_block24_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_conv (Conv2D)   (None, 2, 7, 128)    126976      conv4_block24_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_bn (BatchNormal (None, 2, 7, 128)    512         conv4_block24_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_1_relu (Activatio (None, 2, 7, 128)    0           conv4_block24_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_2_conv (Conv2D)   (None, 2, 7, 32)     36864       conv4_block24_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block24_concat (Concatena (None, 2, 7, 1024)   0           conv4_block23_concat[0][0]       \n",
            "                                                                 conv4_block24_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_bn (BatchNormalization)   (None, 2, 7, 1024)   4096        conv4_block24_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "pool4_relu (Activation)         (None, 2, 7, 1024)   0           pool4_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool4_conv (Conv2D)             (None, 2, 7, 512)    524288      pool4_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool4_pool (AveragePooling2D)   (None, 1, 3, 512)    0           pool4_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 1, 3, 512)    2048        pool4_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_relu (Activation (None, 1, 3, 512)    0           conv5_block1_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 1, 3, 128)    65536       conv5_block1_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 1, 3, 128)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_concat (Concatenat (None, 1, 3, 544)    0           pool4_pool[0][0]                 \n",
            "                                                                 conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_bn (BatchNormali (None, 1, 3, 544)    2176        conv5_block1_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_0_relu (Activation (None, 1, 3, 544)    0           conv5_block2_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 1, 3, 128)    69632       conv5_block2_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 1, 3, 128)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_concat (Concatenat (None, 1, 3, 576)    0           conv5_block1_concat[0][0]        \n",
            "                                                                 conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_bn (BatchNormali (None, 1, 3, 576)    2304        conv5_block2_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_0_relu (Activation (None, 1, 3, 576)    0           conv5_block3_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 1, 3, 128)    73728       conv5_block3_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 1, 3, 128)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_concat (Concatenat (None, 1, 3, 608)    0           conv5_block2_concat[0][0]        \n",
            "                                                                 conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_bn (BatchNormali (None, 1, 3, 608)    2432        conv5_block3_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_0_relu (Activation (None, 1, 3, 608)    0           conv5_block4_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_conv (Conv2D)    (None, 1, 3, 128)    77824       conv5_block4_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_1_relu (Activation (None, 1, 3, 128)    0           conv5_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block4_concat (Concatenat (None, 1, 3, 640)    0           conv5_block3_concat[0][0]        \n",
            "                                                                 conv5_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_bn (BatchNormali (None, 1, 3, 640)    2560        conv5_block4_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_0_relu (Activation (None, 1, 3, 640)    0           conv5_block5_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_conv (Conv2D)    (None, 1, 3, 128)    81920       conv5_block5_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_1_relu (Activation (None, 1, 3, 128)    0           conv5_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block5_concat (Concatenat (None, 1, 3, 672)    0           conv5_block4_concat[0][0]        \n",
            "                                                                 conv5_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_bn (BatchNormali (None, 1, 3, 672)    2688        conv5_block5_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_0_relu (Activation (None, 1, 3, 672)    0           conv5_block6_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_conv (Conv2D)    (None, 1, 3, 128)    86016       conv5_block6_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_1_relu (Activation (None, 1, 3, 128)    0           conv5_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block6_concat (Concatenat (None, 1, 3, 704)    0           conv5_block5_concat[0][0]        \n",
            "                                                                 conv5_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_bn (BatchNormali (None, 1, 3, 704)    2816        conv5_block6_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_0_relu (Activation (None, 1, 3, 704)    0           conv5_block7_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_conv (Conv2D)    (None, 1, 3, 128)    90112       conv5_block7_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block7_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_1_relu (Activation (None, 1, 3, 128)    0           conv5_block7_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block7_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block7_concat (Concatenat (None, 1, 3, 736)    0           conv5_block6_concat[0][0]        \n",
            "                                                                 conv5_block7_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_bn (BatchNormali (None, 1, 3, 736)    2944        conv5_block7_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_0_relu (Activation (None, 1, 3, 736)    0           conv5_block8_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_conv (Conv2D)    (None, 1, 3, 128)    94208       conv5_block8_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block8_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_1_relu (Activation (None, 1, 3, 128)    0           conv5_block8_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block8_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block8_concat (Concatenat (None, 1, 3, 768)    0           conv5_block7_concat[0][0]        \n",
            "                                                                 conv5_block8_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_bn (BatchNormali (None, 1, 3, 768)    3072        conv5_block8_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_0_relu (Activation (None, 1, 3, 768)    0           conv5_block9_0_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_conv (Conv2D)    (None, 1, 3, 128)    98304       conv5_block9_0_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_bn (BatchNormali (None, 1, 3, 128)    512         conv5_block9_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_1_relu (Activation (None, 1, 3, 128)    0           conv5_block9_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_2_conv (Conv2D)    (None, 1, 3, 32)     36864       conv5_block9_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block9_concat (Concatenat (None, 1, 3, 800)    0           conv5_block8_concat[0][0]        \n",
            "                                                                 conv5_block9_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_bn (BatchNormal (None, 1, 3, 800)    3200        conv5_block9_concat[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_0_relu (Activatio (None, 1, 3, 800)    0           conv5_block10_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_conv (Conv2D)   (None, 1, 3, 128)    102400      conv5_block10_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block10_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block10_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block10_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block10_concat (Concatena (None, 1, 3, 832)    0           conv5_block9_concat[0][0]        \n",
            "                                                                 conv5_block10_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_bn (BatchNormal (None, 1, 3, 832)    3328        conv5_block10_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_0_relu (Activatio (None, 1, 3, 832)    0           conv5_block11_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_conv (Conv2D)   (None, 1, 3, 128)    106496      conv5_block11_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block11_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block11_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block11_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block11_concat (Concatena (None, 1, 3, 864)    0           conv5_block10_concat[0][0]       \n",
            "                                                                 conv5_block11_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_bn (BatchNormal (None, 1, 3, 864)    3456        conv5_block11_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_0_relu (Activatio (None, 1, 3, 864)    0           conv5_block12_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_conv (Conv2D)   (None, 1, 3, 128)    110592      conv5_block12_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block12_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block12_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block12_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block12_concat (Concatena (None, 1, 3, 896)    0           conv5_block11_concat[0][0]       \n",
            "                                                                 conv5_block12_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_bn (BatchNormal (None, 1, 3, 896)    3584        conv5_block12_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_0_relu (Activatio (None, 1, 3, 896)    0           conv5_block13_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_conv (Conv2D)   (None, 1, 3, 128)    114688      conv5_block13_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block13_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block13_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block13_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block13_concat (Concatena (None, 1, 3, 928)    0           conv5_block12_concat[0][0]       \n",
            "                                                                 conv5_block13_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_bn (BatchNormal (None, 1, 3, 928)    3712        conv5_block13_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_0_relu (Activatio (None, 1, 3, 928)    0           conv5_block14_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_conv (Conv2D)   (None, 1, 3, 128)    118784      conv5_block14_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block14_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block14_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block14_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block14_concat (Concatena (None, 1, 3, 960)    0           conv5_block13_concat[0][0]       \n",
            "                                                                 conv5_block14_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_bn (BatchNormal (None, 1, 3, 960)    3840        conv5_block14_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_0_relu (Activatio (None, 1, 3, 960)    0           conv5_block15_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_conv (Conv2D)   (None, 1, 3, 128)    122880      conv5_block15_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block15_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block15_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block15_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block15_concat (Concatena (None, 1, 3, 992)    0           conv5_block14_concat[0][0]       \n",
            "                                                                 conv5_block15_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_bn (BatchNormal (None, 1, 3, 992)    3968        conv5_block15_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_0_relu (Activatio (None, 1, 3, 992)    0           conv5_block16_0_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_conv (Conv2D)   (None, 1, 3, 128)    126976      conv5_block16_0_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_bn (BatchNormal (None, 1, 3, 128)    512         conv5_block16_1_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_1_relu (Activatio (None, 1, 3, 128)    0           conv5_block16_1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_2_conv (Conv2D)   (None, 1, 3, 32)     36864       conv5_block16_1_relu[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block16_concat (Concatena (None, 1, 3, 1024)   0           conv5_block15_concat[0][0]       \n",
            "                                                                 conv5_block16_2_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "bn (BatchNormalization)         (None, 1, 3, 1024)   4096        conv5_block16_concat[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "relu (Activation)               (None, 1, 3, 1024)   0           bn[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 1024)         0           relu[0][0]                       \n",
            "==================================================================================================\n",
            "Total params: 7,037,504\n",
            "Trainable params: 2,889,408\n",
            "Non-trainable params: 4,148,096\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h9B2mnJJtFjK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a72ff6c2-6756-4602-a6ac-c6d7ca725995"
      },
      "source": [
        "model = learn.build_model_densenet(base_model, num_labels)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "densenet121 (Model)          (None, 1024)              7037504   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 7,047,754\n",
            "Trainable params: 2,899,658\n",
            "Non-trainable params: 4,148,096\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m4M6d6pLtFjO",
        "colab": {}
      },
      "source": [
        "learn.compile_model_pretrained_net(model, base_lr)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KdlFGuNRtFjR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "c309a58c-f70d-4b96-eb38-113a1fd98c86"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "path = 'id10001/9mQ11vBs1wc/00004.wav'\n",
        "#render.show_mfccs(path, num_rows, num_columns, num_seconds)\n",
        "\n",
        "data = result_sets[0][1]\n",
        "plt.imshow(data, cmap=cm.gray)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7efe1eece940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TRtT-4xMtFjV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "032744b7-14c8-4e8e-b9ae-005444be4bed"
      },
      "source": [
        "evaluate.evaluate_before_training(model, result_sets[0], result_sets[2]) "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "densenet121 (Model)          (None, 1024)              7037504   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 7,047,754\n",
            "Trainable params: 2,899,658\n",
            "Non-trainable params: 4,148,096\n",
            "_________________________________________________________________\n",
            "25/25 [==============================] - 8s 322ms/step - loss: 5.8077 - accuracy: 0.1263\n",
            "Pre-training accuracy: 12.6289%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kk4Y_NRptFjZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5edb70b6-8940-4606-96a5-10a1567ec315"
      },
      "source": [
        "class_weight = learn.calculate_class_weight(featuresdf)\n",
        "history = learn.train_model_class_weights(model, result_sets, num_epochs, num_batch_size, 'DenseNetMelSpec', 'default', 'DenseNetMelSpec', class_weight=class_weight)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 12.4560 - accuracy: 0.1804\n",
            "Epoch 00001: val_loss improved from inf to 2.34874, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 12.4560 - accuracy: 0.1804 - val_loss: 2.3487 - val_accuracy: 0.2010\n",
            "Epoch 2/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 10.3433 - accuracy: 0.2874\n",
            "Epoch 00002: val_loss improved from 2.34874 to 2.11033, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 10.3433 - accuracy: 0.2874 - val_loss: 2.1103 - val_accuracy: 0.2577\n",
            "Epoch 3/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 9.5879 - accuracy: 0.3454\n",
            "Epoch 00003: val_loss improved from 2.11033 to 2.06838, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 17s 284ms/step - loss: 9.5879 - accuracy: 0.3454 - val_loss: 2.0684 - val_accuracy: 0.3093\n",
            "Epoch 4/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 9.1648 - accuracy: 0.3518\n",
            "Epoch 00004: val_loss improved from 2.06838 to 2.01695, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 17s 278ms/step - loss: 9.1648 - accuracy: 0.3518 - val_loss: 2.0170 - val_accuracy: 0.3247\n",
            "Epoch 5/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 8.6726 - accuracy: 0.4137\n",
            "Epoch 00005: val_loss did not improve from 2.01695\n",
            "60/60 [==============================] - 16s 261ms/step - loss: 8.6726 - accuracy: 0.4137 - val_loss: 2.0320 - val_accuracy: 0.2938\n",
            "Epoch 6/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 7.7389 - accuracy: 0.4845\n",
            "Epoch 00006: val_loss improved from 2.01695 to 1.95982, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 17s 276ms/step - loss: 7.7389 - accuracy: 0.4845 - val_loss: 1.9598 - val_accuracy: 0.3608\n",
            "Epoch 7/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 7.1317 - accuracy: 0.5348\n",
            "Epoch 00007: val_loss did not improve from 1.95982\n",
            "60/60 [==============================] - 16s 263ms/step - loss: 7.1317 - accuracy: 0.5348 - val_loss: 2.0278 - val_accuracy: 0.3454\n",
            "Epoch 8/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 6.3650 - accuracy: 0.6005\n",
            "Epoch 00008: val_loss improved from 1.95982 to 1.81955, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 17s 283ms/step - loss: 6.3650 - accuracy: 0.6005 - val_loss: 1.8196 - val_accuracy: 0.3918\n",
            "Epoch 9/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 6.0160 - accuracy: 0.6018\n",
            "Epoch 00009: val_loss improved from 1.81955 to 1.74247, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 21s 345ms/step - loss: 6.0160 - accuracy: 0.6018 - val_loss: 1.7425 - val_accuracy: 0.4175\n",
            "Epoch 10/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 5.6086 - accuracy: 0.6327\n",
            "Epoch 00010: val_loss improved from 1.74247 to 1.72645, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 18s 302ms/step - loss: 5.6086 - accuracy: 0.6327 - val_loss: 1.7264 - val_accuracy: 0.4330\n",
            "Epoch 11/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 5.2177 - accuracy: 0.6598\n",
            "Epoch 00011: val_loss improved from 1.72645 to 1.65471, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 17s 288ms/step - loss: 5.2177 - accuracy: 0.6598 - val_loss: 1.6547 - val_accuracy: 0.4639\n",
            "Epoch 12/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 4.6145 - accuracy: 0.7062\n",
            "Epoch 00012: val_loss improved from 1.65471 to 1.55173, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 17s 284ms/step - loss: 4.6145 - accuracy: 0.7062 - val_loss: 1.5517 - val_accuracy: 0.4845\n",
            "Epoch 13/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 4.4235 - accuracy: 0.7049\n",
            "Epoch 00013: val_loss did not improve from 1.55173\n",
            "60/60 [==============================] - 16s 263ms/step - loss: 4.4235 - accuracy: 0.7049 - val_loss: 1.7097 - val_accuracy: 0.4330\n",
            "Epoch 14/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 3.8684 - accuracy: 0.7448\n",
            "Epoch 00014: val_loss did not improve from 1.55173\n",
            "60/60 [==============================] - 16s 266ms/step - loss: 3.8684 - accuracy: 0.7448 - val_loss: 1.5999 - val_accuracy: 0.4175\n",
            "Epoch 15/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 3.7177 - accuracy: 0.7655\n",
            "Epoch 00015: val_loss improved from 1.55173 to 1.48842, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 17s 282ms/step - loss: 3.7177 - accuracy: 0.7655 - val_loss: 1.4884 - val_accuracy: 0.4794\n",
            "Epoch 16/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 3.5150 - accuracy: 0.7680\n",
            "Epoch 00016: val_loss did not improve from 1.48842\n",
            "60/60 [==============================] - 16s 266ms/step - loss: 3.5150 - accuracy: 0.7680 - val_loss: 1.5974 - val_accuracy: 0.4639\n",
            "Epoch 17/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 3.0925 - accuracy: 0.8054\n",
            "Epoch 00017: val_loss did not improve from 1.48842\n",
            "60/60 [==============================] - 16s 267ms/step - loss: 3.0925 - accuracy: 0.8054 - val_loss: 1.5989 - val_accuracy: 0.4639\n",
            "Epoch 18/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.9175 - accuracy: 0.8093\n",
            "Epoch 00018: val_loss did not improve from 1.48842\n",
            "60/60 [==============================] - 16s 271ms/step - loss: 2.9175 - accuracy: 0.8093 - val_loss: 1.4990 - val_accuracy: 0.5000\n",
            "Epoch 19/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.8178 - accuracy: 0.8080\n",
            "Epoch 00019: val_loss did not improve from 1.48842\n",
            "60/60 [==============================] - 16s 269ms/step - loss: 2.8178 - accuracy: 0.8080 - val_loss: 1.5921 - val_accuracy: 0.5155\n",
            "Epoch 20/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.3418 - accuracy: 0.8582\n",
            "Epoch 00020: val_loss did not improve from 1.48842\n",
            "60/60 [==============================] - 16s 272ms/step - loss: 2.3418 - accuracy: 0.8582 - val_loss: 1.6117 - val_accuracy: 0.4536\n",
            "Epoch 21/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.2195 - accuracy: 0.8595\n",
            "Epoch 00021: val_loss did not improve from 1.48842\n",
            "60/60 [==============================] - 16s 267ms/step - loss: 2.2195 - accuracy: 0.8595 - val_loss: 1.6112 - val_accuracy: 0.4897\n",
            "Epoch 22/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 2.1265 - accuracy: 0.8737\n",
            "Epoch 00022: val_loss did not improve from 1.48842\n",
            "60/60 [==============================] - 16s 269ms/step - loss: 2.1265 - accuracy: 0.8737 - val_loss: 1.5899 - val_accuracy: 0.5000\n",
            "Epoch 23/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.9359 - accuracy: 0.8789\n",
            "Epoch 00023: val_loss did not improve from 1.48842\n",
            "60/60 [==============================] - 16s 267ms/step - loss: 1.9359 - accuracy: 0.8789 - val_loss: 1.6256 - val_accuracy: 0.5000\n",
            "Epoch 24/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.6003 - accuracy: 0.8892\n",
            "Epoch 00024: val_loss did not improve from 1.48842\n",
            "60/60 [==============================] - 16s 259ms/step - loss: 1.6003 - accuracy: 0.8892 - val_loss: 1.5204 - val_accuracy: 0.4639\n",
            "Epoch 25/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.9926 - accuracy: 0.8840\n",
            "Epoch 00025: val_loss did not improve from 1.48842\n",
            "60/60 [==============================] - 15s 258ms/step - loss: 1.9926 - accuracy: 0.8840 - val_loss: 1.5671 - val_accuracy: 0.5206\n",
            "Epoch 26/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.8444 - accuracy: 0.8827\n",
            "Epoch 00026: val_loss improved from 1.48842 to 1.45471, saving model to saved_models/weights.best.DenseNetMelSpec.hdf5\n",
            "60/60 [==============================] - 16s 275ms/step - loss: 1.8444 - accuracy: 0.8827 - val_loss: 1.4547 - val_accuracy: 0.5206\n",
            "Epoch 27/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.5535 - accuracy: 0.8995\n",
            "Epoch 00027: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 269ms/step - loss: 1.5535 - accuracy: 0.8995 - val_loss: 1.5609 - val_accuracy: 0.5206\n",
            "Epoch 28/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.2404 - accuracy: 0.9162\n",
            "Epoch 00028: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 264ms/step - loss: 1.2404 - accuracy: 0.9162 - val_loss: 1.5261 - val_accuracy: 0.5052\n",
            "Epoch 29/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.2519 - accuracy: 0.9137\n",
            "Epoch 00029: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 266ms/step - loss: 1.2519 - accuracy: 0.9137 - val_loss: 1.7098 - val_accuracy: 0.4845\n",
            "Epoch 30/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.3434 - accuracy: 0.9188\n",
            "Epoch 00030: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 264ms/step - loss: 1.3434 - accuracy: 0.9188 - val_loss: 1.6201 - val_accuracy: 0.5155\n",
            "Epoch 31/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.0269 - accuracy: 0.9356\n",
            "Epoch 00031: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 263ms/step - loss: 1.0269 - accuracy: 0.9356 - val_loss: 1.6097 - val_accuracy: 0.5206\n",
            "Epoch 32/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.1978 - accuracy: 0.9214\n",
            "Epoch 00032: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 262ms/step - loss: 1.1978 - accuracy: 0.9214 - val_loss: 1.5829 - val_accuracy: 0.5155\n",
            "Epoch 33/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.1612 - accuracy: 0.9188\n",
            "Epoch 00033: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 265ms/step - loss: 1.1612 - accuracy: 0.9188 - val_loss: 1.5864 - val_accuracy: 0.5155\n",
            "Epoch 34/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.8130 - accuracy: 0.9510\n",
            "Epoch 00034: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 262ms/step - loss: 0.8130 - accuracy: 0.9510 - val_loss: 1.5689 - val_accuracy: 0.5155\n",
            "Epoch 35/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7986 - accuracy: 0.9485\n",
            "Epoch 00035: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 258ms/step - loss: 0.7986 - accuracy: 0.9485 - val_loss: 1.7122 - val_accuracy: 0.4845\n",
            "Epoch 36/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7785 - accuracy: 0.9601\n",
            "Epoch 00036: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 260ms/step - loss: 0.7785 - accuracy: 0.9601 - val_loss: 1.5867 - val_accuracy: 0.5412\n",
            "Epoch 37/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 1.0925 - accuracy: 0.9381\n",
            "Epoch 00037: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 17s 275ms/step - loss: 1.0925 - accuracy: 0.9381 - val_loss: 1.6672 - val_accuracy: 0.5464\n",
            "Epoch 38/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.8593 - accuracy: 0.9472\n",
            "Epoch 00038: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 17s 285ms/step - loss: 0.8593 - accuracy: 0.9472 - val_loss: 1.5922 - val_accuracy: 0.5464\n",
            "Epoch 39/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7414 - accuracy: 0.9485\n",
            "Epoch 00039: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 17s 284ms/step - loss: 0.7414 - accuracy: 0.9485 - val_loss: 1.5250 - val_accuracy: 0.5206\n",
            "Epoch 40/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6310 - accuracy: 0.9626\n",
            "Epoch 00040: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 263ms/step - loss: 0.6310 - accuracy: 0.9626 - val_loss: 1.6981 - val_accuracy: 0.5258\n",
            "Epoch 41/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5071 - accuracy: 0.9729\n",
            "Epoch 00041: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 259ms/step - loss: 0.5071 - accuracy: 0.9729 - val_loss: 1.8274 - val_accuracy: 0.5206\n",
            "Epoch 42/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5713 - accuracy: 0.9626\n",
            "Epoch 00042: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 261ms/step - loss: 0.5713 - accuracy: 0.9626 - val_loss: 1.8932 - val_accuracy: 0.5258\n",
            "Epoch 43/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.7227 - accuracy: 0.9523\n",
            "Epoch 00043: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 261ms/step - loss: 0.7227 - accuracy: 0.9523 - val_loss: 1.8007 - val_accuracy: 0.5000\n",
            "Epoch 44/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6322 - accuracy: 0.9652\n",
            "Epoch 00044: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 258ms/step - loss: 0.6322 - accuracy: 0.9652 - val_loss: 1.6601 - val_accuracy: 0.5206\n",
            "Epoch 45/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.6104 - accuracy: 0.9575\n",
            "Epoch 00045: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 265ms/step - loss: 0.6104 - accuracy: 0.9575 - val_loss: 1.6746 - val_accuracy: 0.5464\n",
            "Epoch 46/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.9601\n",
            "Epoch 00046: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 19s 317ms/step - loss: 0.5748 - accuracy: 0.9601 - val_loss: 1.6242 - val_accuracy: 0.5464\n",
            "Epoch 47/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4077 - accuracy: 0.9691\n",
            "Epoch 00047: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 264ms/step - loss: 0.4077 - accuracy: 0.9691 - val_loss: 1.6742 - val_accuracy: 0.5412\n",
            "Epoch 48/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5074 - accuracy: 0.9716\n",
            "Epoch 00048: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 265ms/step - loss: 0.5074 - accuracy: 0.9716 - val_loss: 1.5952 - val_accuracy: 0.5773\n",
            "Epoch 49/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5054 - accuracy: 0.9652\n",
            "Epoch 00049: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 268ms/step - loss: 0.5054 - accuracy: 0.9652 - val_loss: 1.7332 - val_accuracy: 0.5361\n",
            "Epoch 50/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.9794\n",
            "Epoch 00050: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 265ms/step - loss: 0.3792 - accuracy: 0.9794 - val_loss: 1.7219 - val_accuracy: 0.5258\n",
            "Epoch 51/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.9704\n",
            "Epoch 00051: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 267ms/step - loss: 0.5380 - accuracy: 0.9704 - val_loss: 1.6949 - val_accuracy: 0.5052\n",
            "Epoch 52/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5079 - accuracy: 0.9691\n",
            "Epoch 00052: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 266ms/step - loss: 0.5079 - accuracy: 0.9691 - val_loss: 1.6967 - val_accuracy: 0.5206\n",
            "Epoch 53/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4324 - accuracy: 0.9768\n",
            "Epoch 00053: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 265ms/step - loss: 0.4324 - accuracy: 0.9768 - val_loss: 1.8382 - val_accuracy: 0.5103\n",
            "Epoch 54/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3096 - accuracy: 0.9807\n",
            "Epoch 00054: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 265ms/step - loss: 0.3096 - accuracy: 0.9807 - val_loss: 1.8095 - val_accuracy: 0.5052\n",
            "Epoch 55/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5967 - accuracy: 0.9639\n",
            "Epoch 00055: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 265ms/step - loss: 0.5967 - accuracy: 0.9639 - val_loss: 1.8407 - val_accuracy: 0.4845\n",
            "Epoch 56/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4506 - accuracy: 0.9691\n",
            "Epoch 00056: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 17s 275ms/step - loss: 0.4506 - accuracy: 0.9691 - val_loss: 1.7912 - val_accuracy: 0.5258\n",
            "Epoch 57/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5869 - accuracy: 0.9678\n",
            "Epoch 00057: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 273ms/step - loss: 0.5869 - accuracy: 0.9678 - val_loss: 1.7978 - val_accuracy: 0.5155\n",
            "Epoch 58/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3866 - accuracy: 0.9768\n",
            "Epoch 00058: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 265ms/step - loss: 0.3866 - accuracy: 0.9768 - val_loss: 1.7847 - val_accuracy: 0.5258\n",
            "Epoch 59/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3399 - accuracy: 0.9794\n",
            "Epoch 00059: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 263ms/step - loss: 0.3399 - accuracy: 0.9794 - val_loss: 1.7229 - val_accuracy: 0.5464\n",
            "Epoch 60/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5253 - accuracy: 0.9665\n",
            "Epoch 00060: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 267ms/step - loss: 0.5253 - accuracy: 0.9665 - val_loss: 1.7332 - val_accuracy: 0.5567\n",
            "Epoch 61/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3178 - accuracy: 0.9781\n",
            "Epoch 00061: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 259ms/step - loss: 0.3178 - accuracy: 0.9781 - val_loss: 1.9091 - val_accuracy: 0.5206\n",
            "Epoch 62/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.9704\n",
            "Epoch 00062: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 262ms/step - loss: 0.3848 - accuracy: 0.9704 - val_loss: 1.8985 - val_accuracy: 0.5103\n",
            "Epoch 63/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3936 - accuracy: 0.9768\n",
            "Epoch 00063: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 257ms/step - loss: 0.3936 - accuracy: 0.9768 - val_loss: 1.7298 - val_accuracy: 0.5722\n",
            "Epoch 64/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.5071 - accuracy: 0.9729\n",
            "Epoch 00064: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 254ms/step - loss: 0.5071 - accuracy: 0.9729 - val_loss: 1.8680 - val_accuracy: 0.5361\n",
            "Epoch 65/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4450 - accuracy: 0.9794\n",
            "Epoch 00065: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 253ms/step - loss: 0.4450 - accuracy: 0.9794 - val_loss: 1.7639 - val_accuracy: 0.5361\n",
            "Epoch 66/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.9768\n",
            "Epoch 00066: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 255ms/step - loss: 0.3679 - accuracy: 0.9768 - val_loss: 1.8502 - val_accuracy: 0.5309\n",
            "Epoch 67/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4462 - accuracy: 0.9678\n",
            "Epoch 00067: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 256ms/step - loss: 0.4462 - accuracy: 0.9678 - val_loss: 1.8205 - val_accuracy: 0.5103\n",
            "Epoch 68/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3500 - accuracy: 0.9755\n",
            "Epoch 00068: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 262ms/step - loss: 0.3500 - accuracy: 0.9755 - val_loss: 1.8570 - val_accuracy: 0.5155\n",
            "Epoch 69/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9820\n",
            "Epoch 00069: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 258ms/step - loss: 0.2508 - accuracy: 0.9820 - val_loss: 1.9617 - val_accuracy: 0.5052\n",
            "Epoch 70/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.9832\n",
            "Epoch 00070: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 258ms/step - loss: 0.2790 - accuracy: 0.9832 - val_loss: 2.1815 - val_accuracy: 0.5258\n",
            "Epoch 71/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.9820\n",
            "Epoch 00071: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 259ms/step - loss: 0.2597 - accuracy: 0.9820 - val_loss: 1.8278 - val_accuracy: 0.5515\n",
            "Epoch 72/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3653 - accuracy: 0.9807\n",
            "Epoch 00072: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 265ms/step - loss: 0.3653 - accuracy: 0.9807 - val_loss: 1.7830 - val_accuracy: 0.5155\n",
            "Epoch 73/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2468 - accuracy: 0.9871\n",
            "Epoch 00073: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 258ms/step - loss: 0.2468 - accuracy: 0.9871 - val_loss: 1.7347 - val_accuracy: 0.5412\n",
            "Epoch 74/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2456 - accuracy: 0.9845\n",
            "Epoch 00074: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 260ms/step - loss: 0.2456 - accuracy: 0.9845 - val_loss: 1.9223 - val_accuracy: 0.5206\n",
            "Epoch 75/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2942 - accuracy: 0.9884\n",
            "Epoch 00075: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 263ms/step - loss: 0.2942 - accuracy: 0.9884 - val_loss: 1.7808 - val_accuracy: 0.5309\n",
            "Epoch 76/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.9820\n",
            "Epoch 00076: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 257ms/step - loss: 0.2624 - accuracy: 0.9820 - val_loss: 2.0575 - val_accuracy: 0.5103\n",
            "Epoch 77/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9845\n",
            "Epoch 00077: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 261ms/step - loss: 0.2615 - accuracy: 0.9845 - val_loss: 1.9070 - val_accuracy: 0.5773\n",
            "Epoch 78/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2332 - accuracy: 0.9832\n",
            "Epoch 00078: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 264ms/step - loss: 0.2332 - accuracy: 0.9832 - val_loss: 1.9587 - val_accuracy: 0.5052\n",
            "Epoch 79/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.9691\n",
            "Epoch 00079: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 266ms/step - loss: 0.3121 - accuracy: 0.9691 - val_loss: 1.7937 - val_accuracy: 0.5515\n",
            "Epoch 80/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2269 - accuracy: 0.9884\n",
            "Epoch 00080: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 255ms/step - loss: 0.2269 - accuracy: 0.9884 - val_loss: 2.0707 - val_accuracy: 0.5309\n",
            "Epoch 81/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1518 - accuracy: 0.9897\n",
            "Epoch 00081: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 256ms/step - loss: 0.1518 - accuracy: 0.9897 - val_loss: 1.8488 - val_accuracy: 0.5258\n",
            "Epoch 82/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9794\n",
            "Epoch 00082: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 258ms/step - loss: 0.2618 - accuracy: 0.9794 - val_loss: 2.0344 - val_accuracy: 0.5155\n",
            "Epoch 83/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3220 - accuracy: 0.9845\n",
            "Epoch 00083: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 259ms/step - loss: 0.3220 - accuracy: 0.9845 - val_loss: 1.9948 - val_accuracy: 0.5567\n",
            "Epoch 84/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2247 - accuracy: 0.9923\n",
            "Epoch 00084: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 19s 317ms/step - loss: 0.2247 - accuracy: 0.9923 - val_loss: 2.0188 - val_accuracy: 0.5412\n",
            "Epoch 85/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9858\n",
            "Epoch 00085: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 256ms/step - loss: 0.1923 - accuracy: 0.9858 - val_loss: 2.1154 - val_accuracy: 0.5258\n",
            "Epoch 86/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4013 - accuracy: 0.9768\n",
            "Epoch 00086: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 15s 255ms/step - loss: 0.4013 - accuracy: 0.9768 - val_loss: 1.8959 - val_accuracy: 0.5309\n",
            "Epoch 87/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.9871\n",
            "Epoch 00087: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 260ms/step - loss: 0.2518 - accuracy: 0.9871 - val_loss: 1.8014 - val_accuracy: 0.5361\n",
            "Epoch 88/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2126 - accuracy: 0.9858\n",
            "Epoch 00088: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 264ms/step - loss: 0.2126 - accuracy: 0.9858 - val_loss: 1.9948 - val_accuracy: 0.4948\n",
            "Epoch 89/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.9871\n",
            "Epoch 00089: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 261ms/step - loss: 0.2686 - accuracy: 0.9871 - val_loss: 2.1365 - val_accuracy: 0.5000\n",
            "Epoch 90/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1944 - accuracy: 0.9884\n",
            "Epoch 00090: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 261ms/step - loss: 0.1944 - accuracy: 0.9884 - val_loss: 2.2372 - val_accuracy: 0.5000\n",
            "Epoch 91/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2118 - accuracy: 0.9845\n",
            "Epoch 00091: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 260ms/step - loss: 0.2118 - accuracy: 0.9845 - val_loss: 2.1582 - val_accuracy: 0.4948\n",
            "Epoch 92/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9845\n",
            "Epoch 00092: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 261ms/step - loss: 0.2110 - accuracy: 0.9845 - val_loss: 2.0780 - val_accuracy: 0.4588\n",
            "Epoch 93/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2194 - accuracy: 0.9858\n",
            "Epoch 00093: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 261ms/step - loss: 0.2194 - accuracy: 0.9858 - val_loss: 1.8647 - val_accuracy: 0.5206\n",
            "Epoch 94/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.9807\n",
            "Epoch 00094: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 265ms/step - loss: 0.2822 - accuracy: 0.9807 - val_loss: 1.7914 - val_accuracy: 0.5412\n",
            "Epoch 95/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9845\n",
            "Epoch 00095: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 263ms/step - loss: 0.2313 - accuracy: 0.9845 - val_loss: 1.9882 - val_accuracy: 0.4897\n",
            "Epoch 96/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2667 - accuracy: 0.9845\n",
            "Epoch 00096: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 259ms/step - loss: 0.2667 - accuracy: 0.9845 - val_loss: 2.0335 - val_accuracy: 0.5412\n",
            "Epoch 97/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3234 - accuracy: 0.9807\n",
            "Epoch 00097: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 262ms/step - loss: 0.3234 - accuracy: 0.9807 - val_loss: 1.9503 - val_accuracy: 0.5412\n",
            "Epoch 98/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.9845\n",
            "Epoch 00098: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 261ms/step - loss: 0.1860 - accuracy: 0.9845 - val_loss: 2.0143 - val_accuracy: 0.5052\n",
            "Epoch 99/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1053 - accuracy: 0.9936\n",
            "Epoch 00099: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 260ms/step - loss: 0.1053 - accuracy: 0.9936 - val_loss: 1.8070 - val_accuracy: 0.5773\n",
            "Epoch 100/100\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.9845\n",
            "Epoch 00100: val_loss did not improve from 1.45471\n",
            "60/60 [==============================] - 16s 262ms/step - loss: 0.2615 - accuracy: 0.9845 - val_loss: 2.0973 - val_accuracy: 0.5206\n",
            "Training completed in time:  0:27:18.968891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I2p-lIKFtFjc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eaffb1bc-b112-4836-8d3c-cb801f55556c"
      },
      "source": [
        "evaluate.evaluate_model(model, 'DenseNetMelSpec', result_sets)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy:  0.9407216310501099\n",
            "Testing Accuracy:  0.5206185579299927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hFZwgnSytFjf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "2190876f-b897-49fb-dad6-41a869df5d1b"
      },
      "source": [
        "evaluate.display_metrics(model, 'DenseNetMelSpec', result_sets)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gdrive/My Drive/Untitled folder/wav/evaluate.py:93: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.25      0.31        16\n",
            "           1       0.50      0.78      0.61        18\n",
            "           2       0.62      0.56      0.59        43\n",
            "           3       0.75      0.30      0.43        20\n",
            "           4       0.23      0.46      0.31        13\n",
            "           5       0.68      0.77      0.72        30\n",
            "           6       0.59      0.71      0.65        14\n",
            "           7       0.37      0.44      0.40        16\n",
            "           8       0.40      0.17      0.24        12\n",
            "           9       0.62      0.42      0.50        12\n",
            "\n",
            "    accuracy                           0.52       194\n",
            "   macro avg       0.52      0.48      0.47       194\n",
            "weighted avg       0.55      0.52      0.51       194\n",
            "\n",
            "Confusion matrix: \n",
            "[[ 4  0  3  1  5  1  0  0  1  1]\n",
            " [ 1 14  3  0  0  0  0  0  0  0]\n",
            " [ 2  8 24  0  4  1  0  4  0  0]\n",
            " [ 1  2  5  6  4  1  0  1  0  0]\n",
            " [ 0  2  2  0  6  0  1  0  1  1]\n",
            " [ 0  0  0  0  1 23  2  4  0  0]\n",
            " [ 0  0  0  1  0  1 10  2  0  0]\n",
            " [ 0  1  0  0  1  5  2  7  0  0]\n",
            " [ 2  0  2  0  3  2  0  0  2  1]\n",
            " [ 0  1  0  0  2  0  2  1  1  5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aQL9Ie5StFji",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "ec736cc0-04a1-461c-a5fd-a2a0439e0fb6"
      },
      "source": [
        "evaluate.plot_history(history)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}